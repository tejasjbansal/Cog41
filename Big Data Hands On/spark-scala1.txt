1. To Login
-----------
$ spark-shell


reate a rdd through a collection
---------------------------------
val myarray = Array(1,2,3,4,5)
println(myarray.toList)

println(myarray(0));

for (i <- 0 to (myarray.length-1)) {
     | total += myarray(i);
     | }; println(total)

scala> var max = myarray(0)
max: Int = 1

scala> for (i <- 0 to (myarray.length-1)) {
     | if (myarray(i) > max) max = myarray(i); 
     | }; println(max)




val myData = sc.parallelize(myarray,2) //parallelize will create a RDD on collection
myData.partitions.length   ----number of partitions  = number of tasks
myData.count()


2. Convert text to lower case
------------------------------
val inputfile = sc.textFile("file:/home/cloudera/sample.txt"); -- Base RDD

val inputfile1 = sc.textFile("hdfs://localhost:8020/user/ubh01/emp1.txt");


val lower = inputfile.map(line => line.toLowerCase());

val upper = inputfile.map(a => a.toUpperCase());

lower.foreach(println);

inputfile.foreach(println);

3. Convert text to upper case
------------------------------
val inputfile = sc.textFile("/home/ubh01/sample.txt");

val upper = inputfile.map(a => a.toUpperCase());


upper.foreach(println);

4. Calc the length of a file
-----------------------------
val lines = sc.textFile("hdfs://localhost:8020/user/cloudera/sample.txt");

val linelength = lines.map(s => s.length);


OR

val linelength = lines.map(_.length);

linelength.foreach(println);

val totallength = linelength.reduce((a,b) => a+b);

val maxlength = linelength.reduce((a,b) => a max b);
val minlength = linelength.reduce((a,b) => a min b);



val totallength = linelength.reduce(_+_);

println("Total Length for all characters : " + totallength);



5. Word Count using spark
-------------------------
val inputfile = sc.textFile("hdfs://cloudera-server.com:8020/cognizant/data", 1);

val inputfile = sc.textFile("hdfs://localhost:8020/user/cloudera/sample.txt");


inputfile.foreach(println);

val transform = inputfile.map(a => a.split(" "));

